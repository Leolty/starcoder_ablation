{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "def normalize_empty_lines(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize consecutive empty lines in a string to a maximum of two.\n",
    "\n",
    "    Args:\n",
    "        code (str): Code to normalize.\n",
    "    \n",
    "    Returns:\n",
    "        str: Normalized code.\n",
    "    \"\"\"\n",
    "    normalized_code = re.sub(r'\\n{4,}', '\\n\\n', code)\n",
    "    return normalized_code\n",
    "\n",
    "def construct_prompt(\n",
    "    data: dict, \n",
    "    version: str = \"special\",\n",
    "    repo_token: str = \"<repo_name>\",\n",
    "    file_token: str = \"<file_sep>\",\n",
    "    fim: bool = False,\n",
    "    language: str = \"python\",\n",
    "    tokenizer= None,\n",
    "    max_prompt_length = None\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a prompt for the specified model version.\n",
    "\n",
    "    Args:\n",
    "        data: the data to construct the prompt from\n",
    "        version: 'special', 'normal' or 'baseline'\n",
    "        repo_token: the token to use for the repo name\n",
    "        file_token: the token to use for the file path\n",
    "        fim: whether to use FIM (Fill-In-the-Middle) or not\n",
    "        tokenizer: the tokenizer to use for tokenizing the prompt if specified\n",
    "        max_prompt_length: the maximum length of the prompt if specified\n",
    "    \n",
    "    Returns:\n",
    "        prompt: the constructed prompt or a list of prompts if version is 'all'\n",
    "    \"\"\"\n",
    "    \n",
    "    assert version in [\"special\", \"normal\", \"baseline\"], \"version must be one of ['special', 'normal', 'baseline']\"\n",
    "    assert language in [\"python\", \"java\"], \"language must be one of ['python', 'java']\"\n",
    "    assert tokenizer is not None, \"tokenizer must be specified\"\n",
    "    assert max_prompt_length is not None, \"max_prompt_length must be specified\"\n",
    "\n",
    "    \n",
    "    repo_name = data['repo_name']\n",
    "    file_path = data['file_path']\n",
    "    code = data['cropped_code']\n",
    "    import_statement = data['import_statement']\n",
    "\n",
    "    # special token version\n",
    "    if version == \"special\":\n",
    "        repo_prompt = f\"{repo_token}{repo_name}\"\n",
    "        for snippet in data['context']:\n",
    "            repo_prompt += f\"{file_token}{snippet['path']}\\n{snippet['snippet']}\"\n",
    "            \n",
    "        if fim:\n",
    "            in_file_prompt = f\"{file_token}<fim_prefix>{file_path}\\n{import_statement}\\n{code}<fim_suffix><fim_middle>\"\n",
    "        else:\n",
    "            in_file_prompt = f\"{file_token}{file_path}\\n{import_statement}\\n{code}\"\n",
    "        \n",
    "        if tokenizer is not None and max_prompt_length is not None:\n",
    "            repo_prompt_token_num = len(tokenizer.encode(repo_prompt))\n",
    "            in_file_prompt_token_num = len(tokenizer.encode(in_file_prompt))\n",
    "            \n",
    "            extra_token_num = repo_prompt_token_num + in_file_prompt_token_num - max_prompt_length\n",
    "            if extra_token_num > 0:\n",
    "                # split the repo prompt by lines\n",
    "                repo_prompt_lines = repo_prompt.split(\"\\n\")\n",
    "                # drop lines from end until the extra token number is less than 0\n",
    "                for i in range(len(repo_prompt_lines)-1, -1, -1):\n",
    "                    extra_token_num -= len(tokenizer.encode(repo_prompt_lines[i]))\n",
    "                    if extra_token_num < 0:\n",
    "                        break\n",
    "                \n",
    "                # join the lines back\n",
    "                repo_prompt = \"\\n\".join(repo_prompt_lines[:i+1])\n",
    "            \n",
    "        prompt = repo_prompt + in_file_prompt\n",
    "    \n",
    "    # normal version\n",
    "    elif version == \"normal\":\n",
    "        comment_symbol = \"#\" if language == \"python\" else \"//\"\n",
    "        repo_prompt = f\"{comment_symbol} Repo Name: {data['repo_name']}\\n\"\n",
    "        for snippet in data['context']:\n",
    "            repo_prompt += f\"{comment_symbol} Path: {snippet['path']}\\n{snippet['snippet']}\" + \"\\n\"\n",
    "        \n",
    "        if fim:\n",
    "            in_file_prompt = f\"<fim_prefix>{comment_symbol} Path: {file_path}\\n{import_statement}\\n{code}<fim_suffix><fim_middle>\"\n",
    "        else:\n",
    "            in_file_prompt = f\"{comment_symbol} Path: {file_path}\\n{import_statement}\\n{code}\"\n",
    "        \n",
    "        repo_prompt_token_num = len(tokenizer.encode(repo_prompt))\n",
    "        in_file_prompt_token_num = len(tokenizer.encode(in_file_prompt))\n",
    "        \n",
    "        extra_token_num = repo_prompt_token_num + in_file_prompt_token_num - max_prompt_length\n",
    "        if extra_token_num > 0:\n",
    "            # split the repo prompt by lines\n",
    "            repo_prompt_lines = repo_prompt.split(\"\\n\")\n",
    "            # drop lines from end until the extra token number is less than 0\n",
    "            for i in range(len(repo_prompt_lines)-1, -1, -1):\n",
    "                extra_token_num -= len(tokenizer.encode(repo_prompt_lines[i]))\n",
    "                if extra_token_num < 0:\n",
    "                    break\n",
    "            \n",
    "            # join the lines back\n",
    "            repo_prompt = \"\\n\".join(repo_prompt_lines[:i+1])+ \"\\n\"\n",
    "            \n",
    "        prompt = repo_prompt + in_file_prompt\n",
    "    \n",
    "    # baseline version\n",
    "    elif version == \"baseline\":\n",
    "        comment_symbol = \"#\" if language == \"python\" else \"//\"\n",
    "        in_file_prompt = f\"{comment_symbol} Path: {file_path}\\n{import_statement}\\n{code}\"\n",
    "        \n",
    "        if fim:\n",
    "            in_file_prompt = f\"<fim_prefix>{in_file_prompt}<fim_suffix><fim_middle>\"\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        prompt = in_file_prompt\n",
    "        \n",
    "    return normalize_empty_lines(prompt)\n",
    "\n",
    "def get_first_line_not_comment(code:str, language:str=\"python\"):\n",
    "    \"\"\"\n",
    "    This function gets the first line of code that is not a comment.\n",
    "\n",
    "    Args:\n",
    "    code: Str, the code\n",
    "\n",
    "    Returns:\n",
    "    Str, the first line of code that is not a comment or the first line of code if there is no line that is not a comment\n",
    "    \"\"\"\n",
    "\n",
    "    # check if the language is valid\n",
    "    assert language in [\"python\", \"java\"], \"language must be one of [python, java]\"\n",
    "\n",
    "\n",
    "    # first remove the \\n at the beginning of the code\n",
    "    code = code.lstrip('\\n')\n",
    "\n",
    "    lines = code.split('\\n')\n",
    "    in_multiline_comment = False\n",
    "\n",
    "    if language == \"python\":\n",
    "        for line in lines:\n",
    "            # if the line is empty, then skip\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            # if the line is a start of a multiline comment, then set the in_multiline_comment to True and skip\n",
    "            if not in_multiline_comment and (line.strip().startswith('\"\"\"') or line.strip().startswith(\"'''\")):\n",
    "                in_multiline_comment = True\n",
    "                continue\n",
    "            # if the line is the end of a multiline comment, then set the in_multiline_comment to False and skip\n",
    "            if in_multiline_comment and (line.strip().endswith('\"\"\"') or line.strip().endswith(\"'''\")):\n",
    "                in_multiline_comment = False\n",
    "                continue\n",
    "            # if the line is in a multiline comment, then skip\n",
    "            if in_multiline_comment:\n",
    "                continue\n",
    "            # if the line is a single line comment, then skip\n",
    "            if line.strip().startswith('#'):\n",
    "                continue\n",
    "            # if the line is not a comment, then return the line\n",
    "            return line\n",
    "        \n",
    "    elif language == \"java\":\n",
    "        for line in lines:\n",
    "            # if the line is empty, then skip\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            # if the line is a start of a multiline comment, then set the in_multiline_comment to True and skip\n",
    "            if not in_multiline_comment and line.strip().startswith('/*'):\n",
    "                in_multiline_comment = True\n",
    "                continue\n",
    "            # if the line is the end of a multiline comment, then set the in_multiline_comment to False and skip\n",
    "            if in_multiline_comment and line.strip().endswith('*/'):\n",
    "                in_multiline_comment = False\n",
    "                continue\n",
    "            # if the line is in a multiline comment, then skip\n",
    "            if in_multiline_comment:\n",
    "                continue\n",
    "            # if the line is a single line comment, then skip\n",
    "            if line.strip().startswith('//'):\n",
    "                continue\n",
    "            # if the line is not a comment, then return the line\n",
    "            return line\n",
    "\n",
    "\n",
    "    # if we cannot find a line that is not a comment, then return the first line\n",
    "    return lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tianyang/repobench_python_v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee67ec85c96411c9cfe9a68264f9849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tianyang/anaconda3/envs/repobench/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# load model\n",
    "model_name = \"bigcode/starcoderbase-3b\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\",        \n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = construct_prompt(\n",
    "    dataset['cross_file_first'][31],\n",
    "    version=\"normal\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_prompt_length=7800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Repo Name: see2023/Bert-VITS2-ext\n",
      "# Path: config.py\n",
      "class Resample_config:\n",
      "class Preprocess_text_config:\n",
      "class Bert_gen_config:\n",
      "class Emo_gen_config:\n",
      "class Train_ms_config:\n",
      "class Webui_config:\n",
      "class Server_config:\n",
      "class Translate_config:\n",
      "class Config:\n",
      "    def __init__(self, in_dir: str, out_dir: str, sampling_rate: int = 44100):\n",
      "    def from_dict(cls, dataset_path: str, data: Dict[str, any]):\n",
      "    def __init__(\n",
      "        self,\n",
      "        transcription_path: str,\n",
      "        cleaned_path: str,\n",
      "        train_path: str,\n",
      "        val_path: str,\n",
      "        config_path: str,\n",
      "        val_per_lang: int = 5,\n",
      "        max_val_total: int = 10000,\n",
      "        clean: bool = True,\n",
      "    ):\n",
      "    def from_dict(cls, dataset_path: str, data: Dict[str, any]):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config_path: str,\n",
      "        num_processes: int = 2,\n",
      "        device: str = \"cuda\",\n",
      "        use_multi_device: bool = False,\n",
      "    ):\n",
      "    def from_dict(cls, dataset_path: str, data: Dict[str, any]):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config_path: str,\n",
      "        num_processes: int = 2,\n",
      "        device: str = \"cuda\",\n",
      "        use_multi_device: bool = False,\n",
      "    ):\n",
      "    def from_dict(cls, dataset_path: str, data: Dict[str, any]):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config_path: str,\n",
      "        env: Dict[str, any],\n",
      "        base: Dict[str, any],\n",
      "        model: str,\n",
      "        num_workers: int,\n",
      "        spec_cache: bool,\n",
      "        keep_ckpts: int,\n",
      "    ):\n",
      "    def from_dict(cls, dataset_path: str, data: Dict[str, any]):\n",
      "    def __init__(\n",
      "        self,\n",
      "        device: str,\n",
      "        model: str,\n",
      "        v_model: str,\n",
      "        config_path: str,\n",
      "        language_identification_library: str,\n",
      "        port: int = 7860,\n",
      "        share: bool = False,\n",
      "        debug: bool = False,\n",
      "    ):\n",
      "    def from_dict(cls, dataset_path: str, data: Dict[str, any]):\n",
      "    def __init__(\n",
      "        self, models: List[Dict[str, any]], port: int = 5000, device: str = \"cuda\"\n",
      "    ):\n",
      "    def from_dict(cls, data: Dict[str, any]):\n",
      "    def __init__(self, app_key: str, secret_key: str):\n",
      "    def from_dict(cls, data: Dict[str, any]):\n",
      "    def __init__(self, config_path: str):\n",
      "# Path: text/japanese.py\n",
      "def text2sep_kata(text: str) -> (list, list):\n",
      "    parsed = pyopenjtalk.run_frontend(text)\n",
      "\n",
      "    res = []\n",
      "    sep = []\n",
      "    for parts in parsed:\n",
      "        word, yomi = replace_punctuation(parts[\"string\"]), parts[\"pron\"].replace(\n",
      "            \"’\", \"\"\n",
      "        )\n",
      "        if yomi:\n",
      "            if re.match(_MARKS, yomi):\n",
      "                if len(word) > 1:\n",
      "                    word = [replace_punctuation(i) for i in list(word)]\n",
      "                    yomi = word\n",
      "                    res += yomi\n",
      "                    sep += word\n",
      "                    continue\n",
      "                elif word not in rep_map.keys() and word not in rep_map.values():\n",
      "                    word = \",\"\n",
      "                yomi = word\n",
      "            res.append(yomi)\n",
      "        else:\n",
      "            if word in _SYMBOL_TOKENS:\n",
      "                res.append(word)\n",
      "            elif word in (\"っ\", \"ッ\"):\n",
      "                res.append(\"ッ\")\n",
      "            elif word in _NO_YOMI_TOKENS:\n",
      "                pass\n",
      "            else:\n",
      "                res.append(word)\n",
      "        sep.append(word)\n",
      "    return sep, [hira2kata(i) for i in res], get_accent(parsed)\n",
      "# Path: for_deploy/infer_utils.py\n",
      "import sys\n",
      "import torch\n",
      "from transformers import (\n",
      "    AutoModelForMaskedLM,\n",
      "    AutoTokenizer,\n",
      "    DebertaV2Model,\n",
      "    DebertaV2Tokenizer,\n",
      "    ClapModel,\n",
      "    ClapProcessor,\n",
      ")\n",
      "from config import config\n",
      "from text.japanese import text2sep_kata\n",
      "\n",
      "class BertFeature:\n",
      "    def __init__(self, model_path, language=\"ZH\"):\n",
      "        self.model_path = model_path\n",
      "        self.language = language\n",
      "        self.tokenizer = None\n",
      "        self.model = None\n",
      "        self.device = None\n",
      "\n",
      "        self._prepare()\n",
      "\n",
      "    def _get_device(self, device=config.bert_gen_config.device):\n",
      "        if (\n",
      "            sys.platform == \"darwin\"\n",
      "            and torch.backends.mps.is_available()\n",
      "            and device == \"cpu\"\n",
      "        ):\n",
      "            device = \"mps\"\n",
      "        if not device:\n",
      "            device = \"cuda\"\n",
      "        return device\n",
      "\n",
      "    def _prepare(self):\n",
      "        self.device = self._get_device()\n",
      "\n",
      "        if self.language == \"EN\":\n",
      "            self.tokenizer = DebertaV2Tokenizer.from_pretrained(self.model_path)\n",
      "            self.model = DebertaV2Model.from_pretrained(self.model_path).to(self.device)\n",
      "        else:\n",
      "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
      "            self.model = AutoModelForMaskedLM.from_pretrained(self.model_path).to(\n",
      "                self.device\n",
      "            )\n",
      "        self.model.eval()\n",
      "\n",
      "    def get_bert_feature(self, text, word2ph):\n",
      "        if self.language == \"JP\":\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Path: for_deploy/infer_utils.py\n",
      "import sys\n",
      "import torch\n",
      "from transformers import (\n",
      "    AutoModelForMaskedLM,\n",
      "    AutoTokenizer,\n",
      "    DebertaV2Model,\n",
      "    DebertaV2Tokenizer,\n",
      "    ClapModel,\n",
      "    ClapProcessor,\n",
      ")\n",
      "from config import config\n",
      "from text.japanese import text2sep_kata\n",
      "\n",
      "class BertFeature:\n",
      "    def __init__(self, model_path, language=\"ZH\"):\n",
      "        self.model_path = model_path\n",
      "        self.language = language\n",
      "        self.tokenizer = None\n",
      "        self.model\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "model_outputs = model.generate(**model_inputs, max_new_tokens=128, do_sample=True, temperature=0.2, top_p=0.95)\n",
    "\n",
    "generated_code = tokenizer.decode(model_outputs[0][model_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            sep, ph, accent = text2sep_kata(text)\n",
      "        else:\n",
      "            sep, ph, accent = text.split(), text.split(), None\n",
      "        sep = sep[:config.bert_gen_config.max_sep_len]\n",
      "        sep = sep + [\"[SEP]\"]\n",
      "        sep = sep[:config.bert_gen_config.max_sep_len]\n",
      "        sep = sep + [\"[CLS]\"]\n",
      "        sep = sep[:config.bert_gen_config.max_sep_len]\n",
      "        sep = sep + [\"[SEP]\"]\n",
      "        sep = sep[:config\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(f\"<fim_prefix>{prompt}<fim_suffix><fim_middle>\", return_tensors=\"pt\").to(model.device)\n",
    "model_outputs = model.generate(**model_inputs, max_new_tokens=128, do_sample=True, temperature=0.2, top_p=0.95)\n",
    "\n",
    "generated_code = tokenizer.decode(model_outputs[0][model_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "print(generated_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starcoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
