{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "def normalize_empty_lines(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize consecutive empty lines in a string to a maximum of two.\n",
    "\n",
    "    Args:\n",
    "        code (str): Code to normalize.\n",
    "    \n",
    "    Returns:\n",
    "        str: Normalized code.\n",
    "    \"\"\"\n",
    "    normalized_code = re.sub(r'\\n{4,}', '\\n\\n', code)\n",
    "    return normalized_code\n",
    "\n",
    "def construct_prompt(\n",
    "    data: dict, \n",
    "    version: str = \"special\",\n",
    "    repo_token: str = \"<repo_name>\",\n",
    "    file_token: str = \"<file_sep>\",\n",
    "    fim: bool = False,\n",
    "    language: str = \"python\",\n",
    "    tokenizer= None,\n",
    "    max_prompt_length = None\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a prompt for the specified model version.\n",
    "\n",
    "    Args:\n",
    "        data: the data to construct the prompt from\n",
    "        version: 'special', 'normal' or 'baseline'\n",
    "        repo_token: the token to use for the repo name\n",
    "        file_token: the token to use for the file path\n",
    "        fim: whether to use FIM (Fill-In-the-Middle) or not\n",
    "        tokenizer: the tokenizer to use for tokenizing the prompt if specified\n",
    "        max_prompt_length: the maximum length of the prompt if specified\n",
    "    \n",
    "    Returns:\n",
    "        prompt: the constructed prompt or a list of prompts if version is 'all'\n",
    "    \"\"\"\n",
    "    \n",
    "    assert version in [\"special\", \"normal\", \"baseline\"], \"version must be one of ['special', 'normal', 'baseline']\"\n",
    "    assert language in [\"python\", \"java\"], \"language must be one of ['python', 'java']\"\n",
    "    assert tokenizer is not None, \"tokenizer must be specified\"\n",
    "    assert max_prompt_length is not None, \"max_prompt_length must be specified\"\n",
    "\n",
    "    \n",
    "    repo_name = data['repo_name']\n",
    "    file_path = data['file_path']\n",
    "    code = data['cropped_code']\n",
    "    import_statement = data['import_statement']\n",
    "\n",
    "    # special token version\n",
    "    if version == \"special\":\n",
    "        repo_prompt = f\"{repo_token}{repo_name}\"\n",
    "        for snippet in data['context']:\n",
    "            repo_prompt += f\"{file_token}{snippet['path']}\\n{snippet['snippet']}\"\n",
    "            \n",
    "        if fim:\n",
    "            in_file_prompt = f\"{file_token}<fim_prefix>{file_path}\\n{import_statement}\\n{code}<fim_suffix><fim_middle>\"\n",
    "        else:\n",
    "            in_file_prompt = f\"{file_token}{file_path}\\n{import_statement}\\n{code}\"\n",
    "        \n",
    "        if tokenizer is not None and max_prompt_length is not None:\n",
    "            repo_prompt_token_num = len(tokenizer.encode(repo_prompt))\n",
    "            in_file_prompt_token_num = len(tokenizer.encode(in_file_prompt))\n",
    "            \n",
    "            extra_token_num = repo_prompt_token_num + in_file_prompt_token_num - max_prompt_length\n",
    "            if extra_token_num > 0:\n",
    "                # split the repo prompt by lines\n",
    "                repo_prompt_lines = repo_prompt.split(\"\\n\")\n",
    "                # drop lines from end until the extra token number is less than 0\n",
    "                for i in range(len(repo_prompt_lines)-1, -1, -1):\n",
    "                    extra_token_num -= len(tokenizer.encode(repo_prompt_lines[i]))\n",
    "                    if extra_token_num < 0:\n",
    "                        break\n",
    "                \n",
    "                # join the lines back\n",
    "                repo_prompt = \"\\n\".join(repo_prompt_lines[:i+1])\n",
    "            \n",
    "        prompt = repo_prompt + in_file_prompt\n",
    "    \n",
    "    # normal version\n",
    "    elif version == \"normal\":\n",
    "        comment_symbol = \"#\" if language == \"python\" else \"//\"\n",
    "        repo_prompt = f\"{comment_symbol} Repo Name: {data['repo_name']}\\n\"\n",
    "        for snippet in data['context']:\n",
    "            repo_prompt += f\"{comment_symbol} Path: {snippet['path']}\\n{snippet['snippet']}\" + \"\\n\"\n",
    "        \n",
    "        if fim:\n",
    "            in_file_prompt = f\"<fim_prefix>{comment_symbol} Path: {file_path}\\n{import_statement}\\n{code}<fim_suffix><fim_middle>\"\n",
    "        else:\n",
    "            in_file_prompt = f\"{comment_symbol} Path: {file_path}\\n{import_statement}\\n{code}\"\n",
    "        \n",
    "        repo_prompt_token_num = len(tokenizer.encode(repo_prompt))\n",
    "        in_file_prompt_token_num = len(tokenizer.encode(in_file_prompt))\n",
    "        \n",
    "        extra_token_num = repo_prompt_token_num + in_file_prompt_token_num - max_prompt_length\n",
    "        if extra_token_num > 0:\n",
    "            # split the repo prompt by lines\n",
    "            repo_prompt_lines = repo_prompt.split(\"\\n\")\n",
    "            # drop lines from end until the extra token number is less than 0\n",
    "            for i in range(len(repo_prompt_lines)-1, -1, -1):\n",
    "                extra_token_num -= len(tokenizer.encode(repo_prompt_lines[i]))\n",
    "                if extra_token_num < 0:\n",
    "                    break\n",
    "            \n",
    "            # join the lines back\n",
    "            repo_prompt = \"\\n\".join(repo_prompt_lines[:i+1])+ \"\\n\"\n",
    "            \n",
    "        prompt = repo_prompt + in_file_prompt\n",
    "    \n",
    "    # baseline version\n",
    "    elif version == \"baseline\":\n",
    "        comment_symbol = \"#\" if language == \"python\" else \"//\"\n",
    "        in_file_prompt = f\"{comment_symbol} Path: {file_path}\\n{import_statement}\\n{code}\"\n",
    "        \n",
    "        if fim:\n",
    "            in_file_prompt = f\"<fim_prefix>{in_file_prompt}<fim_suffix><fim_middle>\"\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        prompt = in_file_prompt\n",
    "        \n",
    "    return normalize_empty_lines(prompt)\n",
    "\n",
    "def get_first_line_not_comment(code:str, language:str=\"python\"):\n",
    "    \"\"\"\n",
    "    This function gets the first line of code that is not a comment.\n",
    "\n",
    "    Args:\n",
    "    code: Str, the code\n",
    "\n",
    "    Returns:\n",
    "    Str, the first line of code that is not a comment or the first line of code if there is no line that is not a comment\n",
    "    \"\"\"\n",
    "\n",
    "    # check if the language is valid\n",
    "    assert language in [\"python\", \"java\"], \"language must be one of [python, java]\"\n",
    "\n",
    "\n",
    "    # first remove the \\n at the beginning of the code\n",
    "    code = code.lstrip('\\n')\n",
    "\n",
    "    lines = code.split('\\n')\n",
    "    in_multiline_comment = False\n",
    "\n",
    "    if language == \"python\":\n",
    "        for line in lines:\n",
    "            # if the line is empty, then skip\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            # if the line is a start of a multiline comment, then set the in_multiline_comment to True and skip\n",
    "            if not in_multiline_comment and (line.strip().startswith('\"\"\"') or line.strip().startswith(\"'''\")):\n",
    "                in_multiline_comment = True\n",
    "                continue\n",
    "            # if the line is the end of a multiline comment, then set the in_multiline_comment to False and skip\n",
    "            if in_multiline_comment and (line.strip().endswith('\"\"\"') or line.strip().endswith(\"'''\")):\n",
    "                in_multiline_comment = False\n",
    "                continue\n",
    "            # if the line is in a multiline comment, then skip\n",
    "            if in_multiline_comment:\n",
    "                continue\n",
    "            # if the line is a single line comment, then skip\n",
    "            if line.strip().startswith('#'):\n",
    "                continue\n",
    "            # if the line is not a comment, then return the line\n",
    "            return line\n",
    "        \n",
    "    elif language == \"java\":\n",
    "        for line in lines:\n",
    "            # if the line is empty, then skip\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            # if the line is a start of a multiline comment, then set the in_multiline_comment to True and skip\n",
    "            if not in_multiline_comment and line.strip().startswith('/*'):\n",
    "                in_multiline_comment = True\n",
    "                continue\n",
    "            # if the line is the end of a multiline comment, then set the in_multiline_comment to False and skip\n",
    "            if in_multiline_comment and line.strip().endswith('*/'):\n",
    "                in_multiline_comment = False\n",
    "                continue\n",
    "            # if the line is in a multiline comment, then skip\n",
    "            if in_multiline_comment:\n",
    "                continue\n",
    "            # if the line is a single line comment, then skip\n",
    "            if line.strip().startswith('//'):\n",
    "                continue\n",
    "            # if the line is not a comment, then return the line\n",
    "            return line\n",
    "\n",
    "\n",
    "    # if we cannot find a line that is not a comment, then return the first line\n",
    "    return lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tianyang/repobench_python_v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe30a469a28c4d9fad2c9a0788999980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tianyang/anaconda3/envs/repobench/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# load model\n",
    "model_name = \"bigcode/starcoderbase-3b\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\",        \n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = construct_prompt(\n",
    "    dataset['cross_file_first'][3],\n",
    "    version=\"normal\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_prompt_length=7800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Repo Name: jiawei-ren/dreamgaussian4d\n",
      "# Path: diffusers/src/diffusers/utils/constants.py\n",
      "USE_PEFT_BACKEND = _required_peft_version and _required_transformers_version\n",
      "# Path: diffusers/src/diffusers/models/lora.py\n",
      "class LoRACompatibleLinear(nn.Linear):\n",
      "    \"\"\"\n",
      "    A Linear layer that can be used with LoRA.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, *args, lora_layer: Optional[LoRALinearLayer] = None, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "        self.lora_layer = lora_layer\n",
      "\n",
      "    def set_lora_layer(self, lora_layer: Optional[LoRALinearLayer]):\n",
      "        self.lora_layer = lora_layer\n",
      "\n",
      "    def _fuse_lora(self, lora_scale: float = 1.0, safe_fusing: bool = False):\n",
      "        if self.lora_layer is None:\n",
      "            return\n",
      "\n",
      "        dtype, device = self.weight.data.dtype, self.weight.data.device\n",
      "\n",
      "        w_orig = self.weight.data.float()\n",
      "        w_up = self.lora_layer.up.weight.data.float()\n",
      "        w_down = self.lora_layer.down.weight.data.float()\n",
      "\n",
      "        if self.lora_layer.network_alpha is not None:\n",
      "            w_up = w_up * self.lora_layer.network_alpha / self.lora_layer.rank\n",
      "\n",
      "        fused_weight = w_orig + (lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0])\n",
      "\n",
      "        if safe_fusing and torch.isnan(fused_weight).any().item():\n",
      "            raise ValueError(\n",
      "                \"This LoRA weight seems to be broken. \"\n",
      "                f\"Encountered NaN values when trying to fuse LoRA weights for {self}.\"\n",
      "                \"LoRA weights will not be fused.\"\n",
      "            )\n",
      "\n",
      "        self.weight.data = fused_weight.to(device=device, dtype=dtype)\n",
      "\n",
      "        # we can drop the lora layer now\n",
      "        self.lora_layer = None\n",
      "\n",
      "        # offload the up and down matrices to CPU to not blow the memory\n",
      "        self.w_up = w_up.cpu()\n",
      "        self.w_down = w_down.cpu()\n",
      "        self._lora_scale = lora_scale\n",
      "\n",
      "    def _unfuse_lora(self):\n",
      "        if not (getattr(self, \"w_up\", None) is not None and getattr(self, \"w_down\", None) is not None):\n",
      "            return\n",
      "\n",
      "        fused_weight = self.weight.data\n",
      "        dtype, device = fused_weight.dtype, fused_weight.device\n",
      "\n",
      "        w_up = self.w_up.to(device=device).float()\n",
      "        w_down = self.w_down.to(device).float()\n",
      "\n",
      "        unfused_weight = fused_weight.float() - (self._lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0])\n",
      "        self.weight.data = unfused_weight.to(device=device, dtype=dtype)\n",
      "\n",
      "        self.w_up = None\n",
      "        self.w_down = None\n",
      "\n",
      "    def forward(self, hidden_states: torch.Tensor, scale: float = 1.0) -> torch.Tensor:\n",
      "        if self.lora_layer is None:\n",
      "            out = super().forward(hidden_states)\n",
      "            return out\n",
      "        else:\n",
      "            out = super().forward(hidden_states) + (scale * self.lora_layer(hidden_states))\n",
      "            return out\n",
      "# Path: diffusers/src/diffusers/models/activations.py\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "from torch import nn\n",
      "from ..utils import USE_PEFT_BACKEND\n",
      "from .lora import LoRACompatibleLinear\n",
      "# coding=utf-8\n",
      "# Copyright 2023 HuggingFace Inc.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "ACTIVATION_FUNCTIONS = {\n",
      "    \"swish\": nn.SiLU(),\n",
      "    \"silu\": nn.SiLU(),\n",
      "    \"mish\": nn.Mish(),\n",
      "    \"gelu\": nn.GELU(),\n",
      "    \"relu\": nn.ReLU(),\n",
      "}\n",
      "\n",
      "\n",
      "def get_activation(act_fn: str) -> nn.Module:\n",
      "    \"\"\"Helper function to get activation function from string.\n",
      "\n",
      "    Args:\n",
      "        act_fn (str): Name of activation function.\n",
      "\n",
      "    Returns:\n",
      "        nn.Module: Activation function.\n",
      "    \"\"\"\n",
      "\n",
      "    act_fn = act_fn.lower()\n",
      "    if act_fn in ACTIVATION_FUNCTIONS:\n",
      "        return ACTIVATION_FUNCTIONS[act_fn]\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported activation function: {act_fn}\")\n",
      "\n",
      "\n",
      "class GELU(nn.Module):\n",
      "    r\"\"\"\n",
      "    GELU activation function with tanh approximation support with `approximate=\"tanh\"`.\n",
      "\n",
      "    Parameters:\n",
      "        dim_in (`int`): The number of channels in the input.\n",
      "        dim_out (`int`): The number of channels in the output.\n",
      "        approximate (`str`, *optional*, defaults to `\"none\"`): If `\"tanh\"`, use tanh approximation.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, dim_in: int, dim_out: int, approximate: str = \"none\"):\n",
      "        super().__init__()\n",
      "        self.proj = nn.Linear(dim_in, dim_out)\n",
      "        self.approximate = approximate\n",
      "\n",
      "    def gelu(self, gate: torch.Tensor) -> torch.Tensor:\n",
      "        if gate.device.type != \"mps\":\n",
      "            return F.gelu(gate, approximate=self.approximate)\n",
      "        # mps: gelu is not implemented for float16\n",
      "        return F.gelu(gate.to(dtype=torch.float32), approximate=self.approximate).to(dtype=gate.dtype)\n",
      "\n",
      "    def forward(self, hidden_states):\n",
      "        hidden_states = self.proj(hidden_states)\n",
      "        hidden_states = self.gelu(hidden_states)\n",
      "        return hidden_states\n",
      "\n",
      "\n",
      "class GEGLU(nn.Module):\n",
      "    r\"\"\"\n",
      "    A [variant](https://arxiv.org/abs/2002.05202) of the gated linear unit activation function.\n",
      "\n",
      "    Parameters:\n",
      "        dim_in (`int`): The number of channels in the input.\n",
      "        dim_out (`int`): The number of channels in the output.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, dim_in: int, dim_out: int):\n",
      "        super().__init__()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Repo Name: jiawei-ren/dreamgaussian4d\n",
      "# Path: diffusers/src/diffusers/utils/constants.py\n",
      "USE_PEFT_BACKEND = _required_peft_version and _required_transformers_version\n",
      "# Path: diffusers/src/diffusers/models/lora.py\n",
      "class LoRACompatibleLinear(nn.Linear):\n",
      "    \"\"\"\n",
      "    A Linear layer that can be used with LoRA.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, *args, lora_layer: Optional[LoRALinearLayer] = None, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "        self.lora_layer = lora_layer\n",
      "\n",
      "    def set_lora_layer(self, lora_layer: Optional[LoRALinearLayer]):\n",
      "        self.lora_layer = lora_layer\n",
      "\n",
      "    def _fuse_lora(self, lora_scale: float = 1.0, safe_fusing: bool = False):\n",
      "        if self.lora_layer is None:\n",
      "            return\n",
      "\n",
      "        dtype, device = self.weight.data.dtype, self.weight.data.device\n",
      "\n",
      "        w_orig = self.weight.data.float()\n",
      "        w_up = self.lora_layer.up.weight.data.float()\n",
      "        w_down = self.lora_layer.down.weight.data.float()\n",
      "\n",
      "        if self.lora_layer.network_alpha is not None:\n",
      "            w_up = w_up * self.lora_layer.network_alpha / self.lora_layer.rank\n",
      "\n",
      "        fused_weight = w_orig + (lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0])\n",
      "\n",
      "        if safe_fusing and torch.isnan(fused_weight).any().item():\n",
      "            raise ValueError(\n",
      "                \"This LoRA weight seems to be broken. \"\n",
      "                f\"Encountered NaN values when trying to fuse LoRA weights for {self}.\"\n",
      "                \"LoRA weights will not be fused.\"\n",
      "            )\n",
      "\n",
      "        self.weight.data = fused_weight.to(device=device, dtype=dtype)\n",
      "\n",
      "        # we can drop the lora layer now\n",
      "        self.lora_layer = None\n",
      "\n",
      "        # offload the up and down matrices to CPU to not blow the memory\n",
      "        self.w_up = w_up.cpu()\n",
      "        self.w_down = w_down.cpu()\n",
      "        self._lora_scale = lora_scale\n",
      "\n",
      "    def _unfuse_lora(self):\n",
      "        if not (getattr(self, \"w_up\", None) is not None and getattr(self, \"w_down\", None) is not None):\n",
      "            return\n",
      "\n",
      "        fused_weight = self.weight.data\n",
      "        dtype, device = fused_weight.dtype, fused_weight.device\n",
      "\n",
      "        w_up = self.w_up.to(device=device).float()\n",
      "        w_down = self.w_down.to(device).float()\n",
      "\n",
      "        unfused_weight = fused_weight.float() - (self._lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0])\n",
      "        self.weight.data = unfused_weight.to(device=device, dtype=dtype)\n",
      "\n",
      "        self.w_up = None\n",
      "        self.w_down = None\n",
      "\n",
      "    def forward(self, hidden_states: torch.Tensor, scale: float = 1.0) -> torch.Tensor:\n",
      "        if self.lora_layer is None:\n",
      "            out = super().forward(hidden_states)\n",
      "            return out\n",
      "        else:\n",
      "            out = super().forward(hidden_states) + (scale * self.lora_layer(hidden_states))\n",
      "            return out\n",
      "# Path: diffusers/src/diffusers/models/activations.py\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "from torch import nn\n",
      "from..utils import USE_PEFT_BACKEND\n",
      "from.lora import LoRACompatibleLinear\n",
      "# coding=utf-8\n",
      "# Copyright 2023 HuggingFace Inc.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "ACTIVATION_FUNCTIONS = {\n",
      "    \"swish\": nn.SiLU(),\n",
      "    \"silu\": nn.SiLU(),\n",
      "    \"mish\": nn.Mish(),\n",
      "    \"gelu\": nn.GELU(),\n",
      "    \"relu\": nn.ReLU(),\n",
      "}\n",
      "\n",
      "\n",
      "def get_activation(act_fn: str) -> nn.Module:\n",
      "    \"\"\"Helper function to get activation function from string.\n",
      "\n",
      "    Args:\n",
      "        act_fn (str): Name of activation function.\n",
      "\n",
      "    Returns:\n",
      "        nn.Module: Activation function.\n",
      "    \"\"\"\n",
      "\n",
      "    act_fn = act_fn.lower()\n",
      "    if act_fn in ACTIVATION_FUNCTIONS:\n",
      "        return ACTIVATION_FUNCTIONS[act_fn]\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported activation function: {act_fn}\")\n",
      "\n",
      "\n",
      "class GELU(nn.Module):\n",
      "    r\"\"\"\n",
      "    GELU activation function with tanh approximation support with `approximate=\"tanh\"`.\n",
      "\n",
      "    Parameters:\n",
      "        dim_in (`int`): The number of channels in the input.\n",
      "        dim_out (`int`): The number of channels in the output.\n",
      "        approximate (`str`, *optional*, defaults to `\"none\"`): If `\"tanh\"`, use tanh approximation.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, dim_in: int, dim_out: int, approximate: str = \"none\"):\n",
      "        super().__init__()\n",
      "        self.proj = nn.Linear(dim_in, dim_out)\n",
      "        self.approximate = approximate\n",
      "\n",
      "    def gelu(self, gate: torch.Tensor) -> torch.Tensor:\n",
      "        if gate.device.type!= \"mps\":\n",
      "            return F.gelu(gate, approximate=self.approximate)\n",
      "        # mps: gelu is not implemented for float16\n",
      "        return F.gelu(gate.to(dtype=torch.float32), approximate=self.approximate).to(dtype=gate.dtype)\n",
      "\n",
      "    def forward(self, hidden_states):\n",
      "        hidden_states = self.proj(hidden_states)\n",
      "        hidden_states = self.gelu(hidden_states)\n",
      "        return hidden_states\n",
      "\n",
      "\n",
      "class GEGLU(nn.Module):\n",
      "    r\"\"\"\n",
      "    A [variant](https://arxiv.org/abs/2002.05202) of the gated linear unit activation function.\n",
      "\n",
      "    Parameters:\n",
      "        dim_in (`int`): The number of channels in the input.\n",
      "        dim_out (`int`): The number of channels in the output.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, dim_in: int, dim_out: int):\n",
      "        super().__init__()\n",
      "# Path: diffusers/src/diffusers/models/activations.py\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "from torch import nn\n",
      "from..utils import USE_PEFT_BACKEND\n",
      "from.lora import LoRACompatibleLinear\n",
      "# coding=utf-8\n",
      "# Copyright 2023 HuggingFace Inc.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "model_outputs = model.generate(**model_inputs, max_new_tokens=128, do_sample=True, temperature=0.2, top_p=0.95)\n",
    "\n",
    "generated_code = tokenizer.decode(model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Repo Name: jiawei-ren/dreamgaussian4d\n",
      "# Path: diffusers/src/diffusers/utils/constants.py\n",
      "USE_PEFT_BACKEND = _required_peft_version and _required_transformers_version\n",
      "# Path: diffusers/src/diffusers/models/lora.py\n",
      "class LoRACompatibleLinear(nn.Linear):\n",
      "    \"\"\"\n",
      "    A Linear layer that can be used with LoRA.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, *args, lora_layer: Optional[LoRALinearLayer] = None, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "        self.lora_layer = lora_layer\n",
      "\n",
      "    def set_lora_layer(self, lora_layer: Optional[LoRALinearLayer]):\n",
      "        self.lora_layer = lora_layer\n",
      "\n",
      "    def _fuse_lora(self, lora_scale: float = 1.0, safe_fusing: bool = False):\n",
      "        if self.lora_layer is None:\n",
      "            return\n",
      "\n",
      "        dtype, device = self.weight.data.dtype, self.weight.data.device\n",
      "\n",
      "        w_orig = self.weight.data.float()\n",
      "        w_up = self.lora_layer.up.weight.data.float()\n",
      "        w_down = self.lora_layer.down.weight.data.float()\n",
      "\n",
      "        if self.lora_layer.network_alpha is not None:\n",
      "            w_up = w_up * self.lora_layer.network_alpha / self.lora_layer.rank\n",
      "\n",
      "        fused_weight = w_orig + (lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0])\n",
      "\n",
      "        if safe_fusing and torch.isnan(fused_weight).any().item():\n",
      "            raise ValueError(\n",
      "                \"This LoRA weight seems to be broken. \"\n",
      "                f\"Encountered NaN values when trying to fuse LoRA weights for {self}.\"\n",
      "                \"LoRA weights will not be fused.\"\n",
      "            )\n",
      "\n",
      "        self.weight.data = fused_weight.to(device=device, dtype=dtype)\n",
      "\n",
      "        # we can drop the lora layer now\n",
      "        self.lora_layer = None\n",
      "\n",
      "        # offload the up and down matrices to CPU to not blow the memory\n",
      "        self.w_up = w_up.cpu()\n",
      "        self.w_down = w_down.cpu()\n",
      "        self._lora_scale = lora_scale\n",
      "\n",
      "    def _unfuse_lora(self):\n",
      "        if not (getattr(self, \"w_up\", None) is not None and getattr(self, \"w_down\", None) is not None):\n",
      "            return\n",
      "\n",
      "        fused_weight = self.weight.data\n",
      "        dtype, device = fused_weight.dtype, fused_weight.device\n",
      "\n",
      "        w_up = self.w_up.to(device=device).float()\n",
      "        w_down = self.w_down.to(device).float()\n",
      "\n",
      "        unfused_weight = fused_weight.float() - (self._lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0])\n",
      "        self.weight.data = unfused_weight.to(device=device, dtype=dtype)\n",
      "\n",
      "        self.w_up = None\n",
      "        self.w_down = None\n",
      "\n",
      "    def forward(self, hidden_states: torch.Tensor, scale: float = 1.0) -> torch.Tensor:\n",
      "        if self.lora_layer is None:\n",
      "            out = super().forward(hidden_states)\n",
      "            return out\n",
      "        else:\n",
      "            out = super().forward(hidden_states) + (scale * self.lora_layer(hidden_states))\n",
      "            return out\n",
      "# Path: diffusers/src/diffusers/models/activations.py\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "from torch import nn\n",
      "from..utils import USE_PEFT_BACKEND\n",
      "from.lora import LoRACompatibleLinear\n",
      "# coding=utf-8\n",
      "# Copyright 2023 HuggingFace Inc.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "ACTIVATION_FUNCTIONS = {\n",
      "    \"swish\": nn.SiLU(),\n",
      "    \"silu\": nn.SiLU(),\n",
      "    \"mish\": nn.Mish(),\n",
      "    \"gelu\": nn.GELU(),\n",
      "    \"relu\": nn.ReLU(),\n",
      "}\n",
      "\n",
      "\n",
      "def get_activation(act_fn: str) -> nn.Module:\n",
      "    \"\"\"Helper function to get activation function from string.\n",
      "\n",
      "    Args:\n",
      "        act_fn (str): Name of activation function.\n",
      "\n",
      "    Returns:\n",
      "        nn.Module: Activation function.\n",
      "    \"\"\"\n",
      "\n",
      "    act_fn = act_fn.lower()\n",
      "    if act_fn in ACTIVATION_FUNCTIONS:\n",
      "        return ACTIVATION_FUNCTIONS[act_fn]\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported activation function: {act_fn}\")\n",
      "\n",
      "\n",
      "class GELU(nn.Module):\n",
      "    r\"\"\"\n",
      "    GELU activation function with tanh approximation support with `approximate=\"tanh\"`.\n",
      "\n",
      "    Parameters:\n",
      "        dim_in (`int`): The number of channels in the input.\n",
      "        dim_out (`int`): The number of channels in the output.\n",
      "        approximate (`str`, *optional*, defaults to `\"none\"`): If `\"tanh\"`, use tanh approximation.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, dim_in: int, dim_out: int, approximate: str = \"none\"):\n",
      "        super().__init__()\n",
      "        self.proj = nn.Linear(dim_in, dim_out)\n",
      "        self.approximate = approximate\n",
      "\n",
      "    def gelu(self, gate: torch.Tensor) -> torch.Tensor:\n",
      "        if gate.device.type!= \"mps\":\n",
      "            return F.gelu(gate, approximate=self.approximate)\n",
      "        # mps: gelu is not implemented for float16\n",
      "        return F.gelu(gate.to(dtype=torch.float32), approximate=self.approximate).to(dtype=gate.dtype)\n",
      "\n",
      "    def forward(self, hidden_states):\n",
      "        hidden_states = self.proj(hidden_states)\n",
      "        hidden_states = self.gelu(hidden_states)\n",
      "        return hidden_states\n",
      "\n",
      "\n",
      "class GEGLU(nn.Module):\n",
      "    r\"\"\"\n",
      "    A [variant](https://arxiv.org/abs/2002.05202) of the gated linear unit activation function.\n",
      "\n",
      "    Parameters:\n",
      "        dim_in (`int`): The number of channels in the input.\n",
      "        dim_out (`int`): The number of channels in the output.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, dim_in: int, dim_out: int):\n",
      "        super().__init__()\n",
      "        self.proj = nn.Linear(dim_in, dim_out)\n",
      "\n",
      "    def forward(self, hidden_states):\n",
      "        hidden_states = self.proj(hidden_states)\n",
      "        hidden_states = hidden_states * torch.sigmoid(hidden_states)\n",
      "        return hidden_states\n",
      "\n",
      "\n",
      "class HardSwish(nn.Module):\n",
      "    r\"\"\"\n",
      "    HardSwish activation function.\n",
      "\n",
      "    Parameters:\n",
      "        dim_in (`int`): The number of channels in the input.\n",
      "        dim_out (`int`): The number of channels in the output.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, dim_in\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(f\"<fim_prefix>{prompt}<fim_suffix><fim_middle>\", return_tensors=\"pt\").to(model.device)\n",
    "model_outputs = model.generate(**model_inputs, max_new_tokens=128, do_sample=True, temperature=0.2, top_p=0.95)\n",
    "\n",
    "generated_code = tokenizer.decode(model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starcoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
