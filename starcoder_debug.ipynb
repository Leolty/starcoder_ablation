{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tianyang/repobench_python_v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee67ec85c96411c9cfe9a68264f9849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tianyang/anaconda3/envs/repobench/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# load model\n",
    "model_name = \"bigcode/starcoderbase-3b\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\",        \n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = construct_prompt(\n",
    "    dataset['cross_file_first'][31],\n",
    "    version=\"normal\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_prompt_length=7800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Repo Name: see2023/Bert-VITS2-ext\n",
      "# Path: config.py\n",
      "class Resample_config:\n",
      "class Preprocess_text_config:\n",
      "class Bert_gen_config:\n",
      "class Emo_gen_config:\n",
      "class Train_ms_config:\n",
      "class Webui_config:\n",
      "class Server_config:\n",
      "class Translate_config:\n",
      "class Config:\n",
      "    def __init__(self, in_dir: str, out_dir: str, sampling_rate: int = 44100):\n",
      "    def from_dict(cls, dataset_path: str, data: Dict[str, any]):\n",
      "    def __init__(\n",
      "        self,\n",
      "        transcription_path: str,\n",
      "        cleaned_path: str,\n",
      "        train_path: str,\n",
      "        val_path: str,\n",
      "        config_path: str,\n",
      "        val_per_lang: int = 5,\n",
      "        max_val_total: int = 10000,\n",
      "        clean: bool = True,\n",
      "    ):\n",
      "    def from_dict(cls, dataset_path: str, data: Dict[str, any]):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config_path: str,\n",
      "        num_processes: int = 2,\n",
      "        device: str = \"cuda\",\n",
      "        use_multi_device: bool = False,\n",
      "    ):\n",
      "    def from_dict(cls, dataset_path: str, data: Dict[str, any]):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config_path: str,\n",
      "        num_processes: int = 2,\n",
      "        device: str = \"cuda\",\n",
      "        use_multi_device: bool = False,\n",
      "    ):\n",
      "    def from_dict(cls, dataset_path: str, data: Dict[str, any]):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config_path: str,\n",
      "        env: Dict[str, any],\n",
      "        base: Dict[str, any],\n",
      "        model: str,\n",
      "        num_workers: int,\n",
      "        spec_cache: bool,\n",
      "        keep_ckpts: int,\n",
      "    ):\n",
      "    def from_dict(cls, dataset_path: str, data: Dict[str, any]):\n",
      "    def __init__(\n",
      "        self,\n",
      "        device: str,\n",
      "        model: str,\n",
      "        v_model: str,\n",
      "        config_path: str,\n",
      "        language_identification_library: str,\n",
      "        port: int = 7860,\n",
      "        share: bool = False,\n",
      "        debug: bool = False,\n",
      "    ):\n",
      "    def from_dict(cls, dataset_path: str, data: Dict[str, any]):\n",
      "    def __init__(\n",
      "        self, models: List[Dict[str, any]], port: int = 5000, device: str = \"cuda\"\n",
      "    ):\n",
      "    def from_dict(cls, data: Dict[str, any]):\n",
      "    def __init__(self, app_key: str, secret_key: str):\n",
      "    def from_dict(cls, data: Dict[str, any]):\n",
      "    def __init__(self, config_path: str):\n",
      "# Path: text/japanese.py\n",
      "def text2sep_kata(text: str) -> (list, list):\n",
      "    parsed = pyopenjtalk.run_frontend(text)\n",
      "\n",
      "    res = []\n",
      "    sep = []\n",
      "    for parts in parsed:\n",
      "        word, yomi = replace_punctuation(parts[\"string\"]), parts[\"pron\"].replace(\n",
      "            \"’\", \"\"\n",
      "        )\n",
      "        if yomi:\n",
      "            if re.match(_MARKS, yomi):\n",
      "                if len(word) > 1:\n",
      "                    word = [replace_punctuation(i) for i in list(word)]\n",
      "                    yomi = word\n",
      "                    res += yomi\n",
      "                    sep += word\n",
      "                    continue\n",
      "                elif word not in rep_map.keys() and word not in rep_map.values():\n",
      "                    word = \",\"\n",
      "                yomi = word\n",
      "            res.append(yomi)\n",
      "        else:\n",
      "            if word in _SYMBOL_TOKENS:\n",
      "                res.append(word)\n",
      "            elif word in (\"っ\", \"ッ\"):\n",
      "                res.append(\"ッ\")\n",
      "            elif word in _NO_YOMI_TOKENS:\n",
      "                pass\n",
      "            else:\n",
      "                res.append(word)\n",
      "        sep.append(word)\n",
      "    return sep, [hira2kata(i) for i in res], get_accent(parsed)\n",
      "# Path: for_deploy/infer_utils.py\n",
      "import sys\n",
      "import torch\n",
      "from transformers import (\n",
      "    AutoModelForMaskedLM,\n",
      "    AutoTokenizer,\n",
      "    DebertaV2Model,\n",
      "    DebertaV2Tokenizer,\n",
      "    ClapModel,\n",
      "    ClapProcessor,\n",
      ")\n",
      "from config import config\n",
      "from text.japanese import text2sep_kata\n",
      "\n",
      "class BertFeature:\n",
      "    def __init__(self, model_path, language=\"ZH\"):\n",
      "        self.model_path = model_path\n",
      "        self.language = language\n",
      "        self.tokenizer = None\n",
      "        self.model = None\n",
      "        self.device = None\n",
      "\n",
      "        self._prepare()\n",
      "\n",
      "    def _get_device(self, device=config.bert_gen_config.device):\n",
      "        if (\n",
      "            sys.platform == \"darwin\"\n",
      "            and torch.backends.mps.is_available()\n",
      "            and device == \"cpu\"\n",
      "        ):\n",
      "            device = \"mps\"\n",
      "        if not device:\n",
      "            device = \"cuda\"\n",
      "        return device\n",
      "\n",
      "    def _prepare(self):\n",
      "        self.device = self._get_device()\n",
      "\n",
      "        if self.language == \"EN\":\n",
      "            self.tokenizer = DebertaV2Tokenizer.from_pretrained(self.model_path)\n",
      "            self.model = DebertaV2Model.from_pretrained(self.model_path).to(self.device)\n",
      "        else:\n",
      "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
      "            self.model = AutoModelForMaskedLM.from_pretrained(self.model_path).to(\n",
      "                self.device\n",
      "            )\n",
      "        self.model.eval()\n",
      "\n",
      "    def get_bert_feature(self, text, word2ph):\n",
      "        if self.language == \"JP\":\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Path: for_deploy/infer_utils.py\n",
      "import sys\n",
      "import torch\n",
      "from transformers import (\n",
      "    AutoModelForMaskedLM,\n",
      "    AutoTokenizer,\n",
      "    DebertaV2Model,\n",
      "    DebertaV2Tokenizer,\n",
      "    ClapModel,\n",
      "    ClapProcessor,\n",
      ")\n",
      "from config import config\n",
      "from text.japanese import text2sep_kata\n",
      "\n",
      "class BertFeature:\n",
      "    def __init__(self, model_path, language=\"ZH\"):\n",
      "        self.model_path = model_path\n",
      "        self.language = language\n",
      "        self.tokenizer = None\n",
      "        self.model\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "model_outputs = model.generate(**model_inputs, max_new_tokens=128, do_sample=True, temperature=0.2, top_p=0.95)\n",
    "\n",
    "generated_code = tokenizer.decode(model_outputs[0][model_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            sep, ph, accent = text2sep_kata(text)\n",
      "        else:\n",
      "            sep, ph, accent = text.split(), text.split(), None\n",
      "        sep = sep[:config.bert_gen_config.max_sep_len]\n",
      "        sep = sep + [\"[SEP]\"]\n",
      "        sep = sep[:config.bert_gen_config.max_sep_len]\n",
      "        sep = sep + [\"[CLS]\"]\n",
      "        sep = sep[:config.bert_gen_config.max_sep_len]\n",
      "        sep = sep + [\"[SEP]\"]\n",
      "        sep = sep[:config\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(f\"<fim_prefix>{prompt}<fim_suffix><fim_middle>\", return_tensors=\"pt\").to(model.device)\n",
    "model_outputs = model.generate(**model_inputs, max_new_tokens=128, do_sample=True, temperature=0.2, top_p=0.95)\n",
    "\n",
    "generated_code = tokenizer.decode(model_outputs[0][model_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "print(generated_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starcoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
